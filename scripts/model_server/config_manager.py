#!/usr/bin/env python3
"""
ì„¤ì • ê´€ë¦¬ ëª¨ë“ˆ
"""
import os
import json
import sys


class ConfigManager:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ ë° ê´€ë¦¬"""
    
    def __init__(self, config_file='config.json'):
        self.config_file = config_file
        self.config = self._load_config()
    
    def _load_config(self):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        if not os.path.exists(self.config_file):
            print(f" {self.config_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ê´€ë¦¬ìì—ê²Œ ì„¤ì • íŒŒì¼ì„ ìš”ì²­í•˜ì„¸ìš”.")
            sys.exit(1)
        
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # í•„ìˆ˜ í•­ëª© ì²´í¬
            required_keys = ['aws_access_key', 'aws_secret_key', 'instance_id', 'ssh_key_path']
            for key in required_keys:
                if key not in config or not config[key]:
                    print(f" ì„¤ì • íŒŒì¼ì— {key}ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    sys.exit(1)
            
            # ê¸°ì¡´ ì„¤ì • í˜•ì‹ í˜¸í™˜ì„± ìœ ì§€
            config.setdefault('aws_region', 'us-west-2')
            config.setdefault('ec2_user', 'ubuntu')
            
            # ê¸°ì¡´ ë‹¨ì¼ ëª¨ë¸ ì„¤ì •ì„ ë‹¤ì¤‘ ëª¨ë¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            if 'models' not in config:
                # ê¸°ì¡´ ì„¤ì •ì—ì„œ ê¸°ë³¸ ëª¨ë¸ ìƒì„±
                default_model = {
                    'name': 'Default Model',
                    'path': config.get('model_path', ''),
                    'gpu_layers': config.get('gpu_layers', 32),
                    'threads': config.get('threads', 4),
                    'embedding': True  
                }
                
                config['models'] = {
                    'default': default_model
                }

            if 'base_port' not in config:
                config['base_port'] = config.get('server_port', 8080)
            
            return config
            
        except json.JSONDecodeError as e:
            print(f" ì„¤ì • íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜: {e}")
            sys.exit(1)
        except Exception as e:
            print(f" ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            sys.exit(1)
    
    def get_config(self):
        """ì „ì²´ ì„¤ì • ë°˜í™˜"""
        return self.config
    
    def get_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return self.config.get('models', {})
    
    def add_model(self, model_id, model_info):
        """ìƒˆ ëª¨ë¸ ì¶”ê°€"""
        if 'models' not in self.config:
            self.config['models'] = {}
        
        self.config['models'][model_id] = model_info
        self._save_config()
    
    def _save_config(self):
        """ì„¤ì • íŒŒì¼ ì €ì¥"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f" ì„¤ì • íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    @staticmethod
    def add_model_interactive():
        """ëŒ€í™”í˜•ìœ¼ë¡œ ìƒˆ ëª¨ë¸ ì¶”ê°€"""
        config_file = 'config.json'
        
        if not os.path.exists(config_file):
            print(" config.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            config_manager = ConfigManager(config_file)
            
            print("\n ìƒˆ ëª¨ë¸ ì¶”ê°€")
            print("-" * 30)
            
            model_id = input("ëª¨ë¸ ID (ì˜ˆ: gpt-oss-20b): ").strip()
            if not model_id:
                print(" ëª¨ë¸ IDê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                return
            
            model_name = input("ëª¨ë¸ ì´ë¦„: ").strip() or model_id
            model_path = input("ëª¨ë¸ íŒŒì¼ ê²½ë¡œ: ").strip()
            
            if not model_path:
                print(" ëª¨ë¸ íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                return
            
            gpu_layers = input("GPU ë ˆì´ì–´ ìˆ˜ [32]: ").strip()
            gpu_layers = int(gpu_layers) if gpu_layers else 32
            
            threads = input("ìŠ¤ë ˆë“œ ìˆ˜ [4]: ").strip()
            threads = int(threads) if threads else 4
            
            is_embedding = input("ì„ë² ë”© ëª¨ë¸ì¸ê°€ìš”? (y/n) [n]: ").strip().lower()
            embedding = is_embedding == 'y'
            
            # ìƒˆ ëª¨ë¸ ì¶”ê°€
            model_info = {
                'name': model_name,
                'path': model_path,
                'gpu_layers': gpu_layers,
                'threads': threads,
                'embedding': embedding
            }
            
            config_manager.add_model(model_id, model_info)
            
            print(f"\n ëª¨ë¸ '{model_id}' ì¶”ê°€ ì™„ë£Œ!")
            print("config.json íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f" ëª¨ë¸ ì¶”ê°€ ì‹¤íŒ¨: {e}")
    
    @staticmethod
    def create_template():
        """ì„¤ì • í…œí”Œë¦¿ ìƒì„±"""
        template = {
            "_comment": "AI ë¹Œë“œ ì„œë²„ ì„¤ì • íŒŒì¼ - ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›",
            
            "aws_access_key": "YOUR_AWS_ACCESS_KEY",
            "aws_secret_key": "YOUR_AWS_SECRET_KEY",
            "aws_region": "us-east-1",
            "instance_id": "i-1234567890abcdef0",
            
            "ssh_key_path": "./keys/server-key.pem",
            "ec2_user": "ubuntu",
            
            "base_port": 8080,
            "server_work_dir": "/home/ubuntu/llama.cpp",
            
            "models": {
                "qwen3-embedding": {
                    "name": "Qwen3 Embedding 0.6B",
                    "path": "/home/ubuntu/llama.cpp/models/qwen3-embedding-0.6b/Qwen3-Embedding-0.6B-Q8_0.gguf",
                    "gpu_layers": 32,
                    "threads": 4,
                    "embedding": True
                },
                "gpt-oss-20b": {
                    "name": "GPT OSS 20B",
                    "path": "/home/ubuntu/llama.cpp/models/gpt-oss-20b/gpt-oss-20b-f16.gguf",
                    "gpu_layers": 40,
                    "threads": 4,
                    "embedding": False
                }
            },
            
            # ê¸°ì¡´ ì„¤ì • í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ (deprecated)
            "server_port": 8080,
            "model_path": "/home/ubuntu/llama.cpp/models/qwen3-embedding-0.6b/Qwen3-Embedding-0.6B-Q8_0.gguf",
            "gpu_layers": 32,
            "threads": 4
        }
        
        with open('config.json.template', 'w', encoding='utf-8') as f:
            json.dump(template, f, indent=2, ensure_ascii=False)
        
        print("ë‹¤ì¤‘ ëª¨ë¸ ì§€ì› ì„¤ì • í…œí”Œë¦¿ ìƒì„±: config.json.template")
        print("ê¸°ì¡´ config.jsonê³¼ í˜¸í™˜ë˜ë©°, models ì„¹ì…˜ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥")